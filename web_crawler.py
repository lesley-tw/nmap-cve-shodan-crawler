from selenium import webdriver
import urllib
import sys
import lxml.html as lh
import time
import json

def crawl_web(payload, url):
	driver = webdriver.PhantomJS()
	# get full url
	data = urllib.urlencode(payload)
	full_url = url + str("?") + data
	driver.implicitly_wait(10)
	driver.get(full_url)

	# crawler html
	content = driver.execute_script("return document.documentElement.innerHTML;")
	driver.close()
	with open("requests_results.html", "w") as f:
		f.write(content)
	return content

def nmap_data(content):
	d = lh.fromstring(content)
	a_tags = d.xpath('//td[@class="gsc-table-cell-snippet-close"]/div[@class="gs-title gsc-table-cell-thumbnail gsc-thumbnail-left"]/a[@class="gs-title"]')
	doc_list = list()
	print len(a_tags)
	for a_tag in a_tags:
		tmp_dict = dict()
		vul = a_tag.xpath(".//descendant-or-self::text()")
		vul = ' '.join(vul)
		tmp_dict["vul"] = vul
		link = a_tag.xpath("./@href")
		# print link
		payload = {}
		c = crawl_web(payload, link[0])
		d = lh.fromstring(c)
		description = d.xpath("/html/body/table[2]/tbody/tr[1]/td[2]/table/tbody/tr/td/*[self::pre or self::tt or self::blockquote]/text()")
		description = ' '.join(description)
		# print description
		tmp_dict["description"] = description
		doc_list.append(tmp_dict)
	return doc_list


def cve_data(content):
	doc_list = list()
	d = lh.fromstring(content)
	tr_values = d.xpath('//*[@id="TableWithRules"]/table/tbody/tr')
	print len(tr_values)
	for tr_value in tr_values:
		tmp_dict = dict()
		description = str(tr_value.xpath('normalize-space(.//td[2]/text())'))
		vul = str(tr_value.xpath('normalize-space(.//td[1]/a/text())'))
		# print description
		tmp_dict["Description"] = description
		tmp_dict["vul"] = vul
		doc_list.append(tmp_dict)
	return doc_list


def shodan_data(content):
	doc_list = list()
	d = lh.fromstring(content)
	a_values = d.xpath("//a[contains(@class, 'title')]")
	print len(a_values)
	for a_value in a_values:
		tmp_dict = dict()
		description = a_value.xpath(".//descendant-or-self::text()")
		description = ' '.join(description)
		tmp_dict["Description"] = description
		tmp_dict["vul"] = ""
		doc_list.append(tmp_dict)
	return doc_list


if __name__ == '__main__':
	start_time = time.time()
	reload(sys)
	sys.setdefaultencoding("utf-8")

	query_string = "htpasswd"
	##### crawl nmap
	print "===crawl nmap==="
	payload = {'cx': 'partner-pub-007856554 6631069:bx60rb-fytx', 'cof': 'FORID:9', 'ie': 'ISO-8859-1','q': query_string, 'sa': 'Site Search'}
	url = 'https://nmap.org/search.html'
	content = crawl_web(payload, url)
	nmap_list = nmap_data(content)

	# ##### crawl cve
	print "===crawl cve==="
	payload2 = {'keyword': query_string}
	url2 = 'https://cve.mitre.org/cgi-bin/cvekey.cgi'
	content = crawl_web(payload2, url2)
	cve_list = cve_data(content)

	##### crawl shodan exploits
	print "===crawl shodan==="
	payload3 = {'q': query_string}
	url3 = 'https://exploits.shodan.io'
	content = crawl_web(payload3, url3)
	shodan_list = shodan_data(content)

	document_list = nmap_list + cve_list + shodan_list
	# print document_list
	with open("document.json", 'wb') as outfile:
		json.dump(document_list, outfile, indent=4)
	outfile.close()
	print("--- %s seconds ---" % (time.time() - start_time))

